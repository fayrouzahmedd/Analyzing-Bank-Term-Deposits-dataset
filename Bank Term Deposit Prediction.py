# -*- coding: utf-8 -*-
"""Fayrouz Ahmed 320210131 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fqw-GgRFmXOQFXEyhfziG0zNHbX95sf8
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.impute import KNNImputer
from sklearn.impute import SimpleImputer
from scipy.stats import kurtosis, skew
from scipy.stats import chi2_contingency, ttest_ind, f_oneway
from sklearn.preprocessing import LabelEncoder
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import KFold
from scipy import stats
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
import warnings
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from scipy.linalg import svd

df = pd.read_csv('bank.csv')

#checking 5 rows sample from Dataframes
df.head(5)

#checking Dataframe basic info
df.info()

#checking the number of rows and columns
df.shape

df.describe()

# checking for NaN values
df.isnull().sum()

df.isna().any(axis=0)

# Remove rows with NaN values in any row
df.dropna(axis=0, inplace=True)
df.isna().any(axis=0)

# Remove rows with NaN values in any column
df.dropna(axis=1, inplace=True)
df.isna().any(axis=1)

#fill missing data with unknown
df['age'].fillna('Unknown', inplace=True)
df['balance'].fillna('Unknown', inplace=True)
df['day'].fillna('Unknown', inplace=True)
df['duration'].fillna('Unknown', inplace=True)
df['campaign'].fillna('Unknown', inplace=True)
df['pdays'].fillna('Unknown', inplace=True)
df['previous'].fillna('Unknown', inplace=True)

df.isnull().sum() #shows the count of missing values after filling

df.duplicated().sum()

df.hist(figsize=(10,25))

# create a boxplot of the age
sns.boxplot(df.age)
plt.show()

# Let's say you want to create a bar plot for the 'job' column
job_counts = df['job'].value_counts()  # Count the occurrences of each job

plt.figure(figsize=(10, 6))  # Set the figure size
job_counts.plot(kind='bar', color='skyblue')  # Create the bar plot
plt.title('Counts of Jobs')  # Set the title of the plot
plt.xlabel('Job')  # Set the label for the x-axis
plt.ylabel('Count')  # Set the label for the y-axis

# Define the numerical columns
numerical_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']

# Fill missing values with the mean of each numerical column
for col in numerical_columns:
    df[col].fillna(df[col].mean(), inplace=True)

"""# **COVARIANCE MATRIX**"""

numerical_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']

# Calculate the covariance matrix
covariance_matrix = df[numerical_columns].cov()

# Create a heatmap for the covariance matrix
plt.figure(figsize=(10, 8))
sns.heatmap(covariance_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Covariance Matrix')

"""# **CORRELATION**"""

# Select the numerical columns for which you want to calculate the correlation matrix
numerical_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']

# Calculate the correlation matrix
correlation_matrix = df[numerical_columns].corr()

# Create a heatmap for the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')

"""# **CHI-SQUARE**"""

# CHI SQUARE TEST

# Let's assume we want to perform a chi-square test for the 'marital' and 'loan' columns
observed = pd.crosstab(df['marital'], df['loan'])

# Perform the chi-square test
chi2, p, dof, expected = chi2_contingency(observed)

print("Chi-square statistic:", chi2)
print("P-value:", p)
print("Degrees of freedom:", dof)
print("Expected frequencies:")
print(expected)

# Construct a bar plot for the observed frequencies
observed.plot(kind='bar', stacked=True)
plt.title('Observed Frequencies')
plt.xlabel('Marital Status')
plt.ylabel('Count')

plt.show()

"""# **T-TEST**"""

# Select the numerical columns for the T-test
numerical_col1 = 'duration'
numerical_col2 = 'campaign'

# Drop rows with missing values in the selected numerical columns
data = df.dropna(subset=[numerical_col1, numerical_col2])

# Extract the values for the T-test
values1 = data[numerical_col1]
values2 = data[numerical_col2]

# Perform the T-test
t_test_result = ttest_ind(values1, values2)

# Print the T-test results
print("t-test statistic:", t_test_result.statistic)
print("P-value:", t_test_result.pvalue)

"""# **ANOVA**"""

# Select the numerical column and the categorical column for the ANOVA test
numerical_column = 'age'
categorical_column = 'marital'

# Drop rows with missing values in the selected columns
data = df.dropna(subset=[numerical_column, categorical_column])

# Perform the ANOVA test
groups = [data[data[categorical_column] == group][numerical_column] for group in data[categorical_column].unique()]
f_statistic, p_value = f_oneway(*groups)

# Print the ANOVA test results
print("F-statistic:", f_statistic)
print("P-value:", p_value)

"""# **PCA & LDA**"""

# Encode categorical variables
label_encoder = LabelEncoder()
df['job'] = label_encoder.fit_transform(df['job'])
df['marital'] = label_encoder.fit_transform(df['marital'])
df['education'] = label_encoder.fit_transform(df['education'])
df['default'] = label_encoder.fit_transform(df['default'])
df['housing'] = label_encoder.fit_transform(df['housing'])
df['loan'] = label_encoder.fit_transform(df['loan'])
df['contact'] = label_encoder.fit_transform(df['contact'])
df['month'] = label_encoder.fit_transform(df['month'])
df['deposit'] = label_encoder.fit_transform(df['deposit'])

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(df[['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']])

# Perform Linear Discriminant Analysis (LDA)
lda = LinearDiscriminantAnalysis(n_components=min(X.shape[1], len(set(df['deposit'])) - 1))
X_lda = lda.fit_transform(X, df['deposit'])

# Perform Principal Component Analysis (PCA)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Create a plot for LDA
plt.figure(figsize=(10, 5))
plt.scatter(X_lda, [0] * len(X_lda), c=df['deposit'], cmap='viridis', alpha=0.5)
plt.title('Linear Discriminant Analysis')
plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.colorbar()
plt.show()

# Create a plot for PCA
plt.figure(figsize=(10, 5))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['deposit'], cmap='viridis', alpha=0.5)
plt.title('Principal Component Analysis')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar()
plt.show()

"""# **NAIVE BAYESIAN**"""

X = df[['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']].copy()
y = df['deposit']

# Encoding categorical variables using .loc to avoid SettingWithCopyWarning
#encoder = LabelEncoder()
#X.loc[:, 'Sex'] = encoder.fit_transform(X['Sex'])
#X.loc[:, 'Embarked'] = encoder.fit_transform(X['Embarked'])

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Instantiate the Naive Bayes classifier
naive_bayes = GaussianNB()

# Fit the model on the training data
naive_bayes.fit(X_train, y_train)

# Predict on the test data
y_pred = naive_bayes.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# Classification report
print(classification_report(y_test, y_pred))

# Confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""# **BNN**"""

# Encoding categorical variables
data['job'] = data['job'].map({'management': 0, 'technician': 1, 'blue-collar': 2, 'admin.': 3, 'services': 4, 'retired': 5, 'self-employed': 6, 'entrepreneur': 7, 'unemployed': 8, 'housemaid': 9, 'student': 10})
data['marital'] = data['marital'].map({'single': 0, 'married': 1, 'divorced': 2})
data['education'] = data['education'].map({'primary': 0, 'secondary': 1, 'tertiary': 2})
data['default'] = data['default'].map({'no': 0, 'yes': 1})
data['housing'] = data['housing'].map({'no': 0, 'yes': 1})
data['loan'] = data['loan'].map({'no': 0, 'yes': 1})
data['contact'] = data['contact'].map({'cellular': 0, 'telephone': 1})
data['poutcome'] = data['poutcome'].map({'success': 0, 'failure': 1, 'other': 2, 'unknown': 3})
data['deposit'] = data['deposit'].map({'no': 0, 'yes': 1})

# Define features and target variable
features = ['job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome']
target = 'deposit'

# Calculate probabilities manually (Example based on a subset of features)
# P(deposit | job, marital)
deposit_given_job_marital = data.groupby(['job', 'marital', 'deposit']).size().unstack().fillna(0)
deposit_given_job_marital = deposit_given_job_marital.div(deposit_given_job_marital.sum(axis=1), axis=0)
# P(deposit | education, default)
deposit_given_education_default = data.groupby(['education', 'default', 'deposit']).size().unstack().fillna(0)
deposit_given_education_default = deposit_given_education_default.div(deposit_given_education_default.sum(axis=1), axis=0)
# Define conditional probabilities for other features similarly...

# Example of querying the probabilities
# P(deposit = 1 | job = 0, marital = 1)
prob_deposit_given_job_marital = deposit_given_job_marital.loc[(0, 1), 1]
print(f"Probability of Deposit given job=0 and marital=1: {prob_deposit_given_job_marital}")

"""# **DECISION TREE**"""

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree classifier
dt_classifier = DecisionTreeClassifier()

# Train the Decision Tree classifier
dt_classifier.fit(X_train, y_train)

# Make predictions
y_pred = dt_classifier.predict(X_test)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Print classification report
print(classification_report(y_test, y_pred))

# Print confusion matrix
print(confusion_matrix(y_test, y_pred))

"""# **KNN(DIFFERENT DISTANCES) AND DATA SPLITTING**"""

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the K-NN classifier with different distance metrics
knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')
knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')

# Train the K-NN classifiers
knn_euclidean.fit(X_train, y_train)
knn_manhattan.fit(X_train, y_train)

# Make predictions
y_pred_euclidean = knn_euclidean.predict(X_test)
y_pred_manhattan = knn_manhattan.predict(X_test)

# Evaluate the classifiers
accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)
accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)

print("Accuracy (Euclidean):", accuracy_euclidean)
print("Accuracy (Manhattan):", accuracy_manhattan)

# Print classification report and confusion matrix for Euclidean
print("Classification Report (Euclidean):")
print(classification_report(y_test, y_pred_euclidean))
print("Confusion Matrix (Euclidean):")
print(confusion_matrix(y_test, y_pred_euclidean))

# Print classification report and confusion matrix for Manhattan
print("Classification Report (Manhattan):")
print(classification_report(y_test, y_pred_manhattan))
print("Confusion Matrix (Manhattan):")
print(confusion_matrix(y_test, y_pred_manhattan))

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
# Display evaluation metrics
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Confusion Matrix:\n{conf_matrix}")

"""# **K-fold cross validation and average accuracy**"""

# Assuming X and y are already defined from the previous code
# Initialize KFold and Decision Tree Classifier
kf = KFold(n_splits=5, shuffle=True, random_state=42)
model = DecisionTreeClassifier(random_state=42)
fold = 1
accuracies = []

# Perform K-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)
    print(f"Fold {fold} Accuracy: {accuracy}")
    fold += 1

# Calculate average accuracy
avg_accuracy = sum(accuracies) / len(accuracies)
print("Average accuracy:", avg_accuracy)

"""# **CONFUSION MATRIX**"""

# Assuming X and y are already defined from the previous code
# Assuming model and X_test, y_test are defined from the k-fold cross-validation code

# Make predictions
y_pred = model.predict(X_test)

# Calculate evaluation metrics
conf_matrix = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
error_rate = 1 - accuracy
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Display results
print("Confusion Matrix:")
print(conf_matrix)
print("\nAccuracy:", accuracy)
print("Error Rate:", error_rate)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# Plot confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Plot evaluation metrics
metrics = ['Accuracy', 'Error Rate', 'Precision', 'Recall', 'F1 Score']
values = [accuracy, error_rate, precision, recall, f1]
plt.figure(figsize=(6, 4))
plt.bar(metrics, values, color=['blue', 'red', 'green', 'orange', 'purple'])
plt.title('Evaluation Metrics')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.show()

# Compare training and testing accuracy
train_accuracy = model.score(X_train, y_train)
test_accuracy = model.score(X_test, y_test)
print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

# Compare the confusion matrix to interpret results further
print("Confusion Matrix:")
print(conf_matrix)

model = DecisionTreeClassifier()
model.fit(X_train, y_train)
# Now checking for overfitting or underfitting
train_accuracy = model.score(X_train, y_train)
test_accuracy = model.score(X_test, y_test)
if train_accuracy > test_accuracy:
 print("The model might be overfitting.")
elif train_accuracy < test_accuracy:
 print("The model might be underfitting.")
else:
 print("The model has a balanced fit.")

"""# **NEURAL NETWORKS**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,  random_state=42)
# Scale the features (if not already done)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Define the neural network architecture
model = Sequential()
model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=10)
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')

"""# **ROC**"""

# Mapping categorical variables to numerical values
df['job'] = df['job'].map({'admin.': 0, 'blue-collar': 1, 'entrepreneur': 2, 'housemaid': 3, 'management': 4, 'retired': 5, 'self-employed': 6, 'services': 7, 'student': 8, 'technician': 9, 'unemployed': 10, 'unknown': 11})
df['marital'] = df['marital'].map({'divorced': 0, 'married': 1, 'single': 2, 'unknown': 3})
df['education'] = df['education'].map({'primary': 0, 'secondary': 1, 'tertiary': 2, 'unknown': 3})
df['default'] = df['default'].map({'no': 0, 'yes': 1})
df['housing'] = df['housing'].map({'no': 0, 'yes': 1})
df['loan'] = df['loan'].map({'no': 0, 'yes': 1})
df['contact'] = df['contact'].map({'cellular': 0, 'telephone': 1, 'unknown': 2})
df['month'] = df['month'].map({'jan': 0, 'feb': 1, 'mar': 2, 'apr': 3, 'may': 4, 'jun': 5, 'jul': 6, 'aug': 7, 'sep': 8, 'oct': 9, 'nov': 10, 'dec': 11})
df['poutcome'] = df['poutcome'].map({'failure': 0, 'other': 1, 'success': 2, 'unknown': 3})
df['deposit'] = df['deposit'].map({'no': 0, 'yes': 1})

# Handling missing values
df['age'].fillna(df['age'].mean(), inplace=True)
df['balance'].fillna(df['balance'].mean(), inplace=True)
df['day'].fillna(df['day'].mean(), inplace=True)
df['duration'].fillna(df['duration'].mean(), inplace=True)
df['campaign'].fillna(df['campaign'].mean(), inplace=True)
df['pdays'].fillna(df['pdays'].mean(), inplace=True)
df['previous'].fillna(df['previous'].mean(), inplace=True)

# Define features and target variable
features = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome']
X = df[features]
y = df['deposit']

data = data.dropna(subset=['deposit'])
y = y.dropna()

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
knn = KNeighborsClassifier(n_neighbors=3)

# Cross-validation and ROC curve calculation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
y_pred = cross_val_predict(knn, X_train, y_train, cv=cv)
fpr, tpr, _ = roc_curve(y_train, y_pred, pos_label=1)
roc_auc = auc(fpr, tpr)

# Plotting the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""# **SVD**"""

# Check and drop columns if they exist
columns_to_drop = ['Name', 'Ticket', 'Cabin']
for col in columns_to_drop:
    if col in df.columns:
        df = df.drop(columns=col)

# Proceed with label encoding
label_encoders = {}
for column in ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'deposit']:
    label_encoders[column] = LabelEncoder()
    df[column] = df[column].fillna('Missing')  # Fill missing values
    df[column] = label_encoders[column].fit_transform(df[column])

# Handle missing values in other columns
imputer = SimpleImputer(strategy='mean')
df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Perform SVD
U, S, Vt = svd(df, full_matrices=False)

# Select the number of components to keep
n_components = 5
reduced_data = U[:, :n_components] * S[:n_components]

# Create a reduced DataFrame
reduced_df = pd.DataFrame(reduced_data, columns=[f'Component_{i}' for i in range(n_components)])

print("Shapes of SVD components:", U.shape, S.shape, Vt.shape)
print("Reduced df:\n", reduced_df.head())

"""# **PCA&LDA**"""

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Applying PCA
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Applying LDA with the appropriate number of components
lda = LinearDiscriminantAnalysis(n_components=1)  # Assuming 1 component for example
X_train_lda = lda.fit_transform(X_train_scaled, y_train)
X_test_lda = lda.transform(X_test_scaled)